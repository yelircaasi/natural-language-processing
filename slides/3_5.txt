Hey. We have just covered lots of ways to build word embeddings. But, you know what? Why words? I mean, sometimes we need representations for sentences, and it's not so obvious so far how to get that, and in some cases, we need to go to sub-word level. For example, we might have a language with rich morphology and then it would be nice to somehow use this morphology in our models. Actually, linguistics can be really helpful. So, we will see a couple of examples right here. Let us start with morphology. So, for example, in English you can say mature or immature, and then relevant, irrelevant and so on. So, you know that there are some prefixes that can change the meaning of the word to the opposite one. So we will have antonyms. Now, on the other hand, you can understand that there are some suffixes that do not change the semantics of the words a lot. For example, I have no break and the breaker are still about the similar concepts in some sense. So the idea of the window proposed in the paper in the bottom of the slide is the following. Let us try to put the words that have some not important morphological changes together in the space, and on the opposite, let us try to have the embeddings for words that have some prefixes that change the meaning of the word completely. Let us put them as far as possible. So you see you try to put some words closer and some words more far away. You can do this in many ways like, let's say it would be some regularization of your model, or you will have some loss, and then at some other losses, to make sure that you have this additional constraints. Okay? This idea is nice, but sometimes we don't have linguists to tell us what is the morphological patterns in the language. What can we do then? Well, in this case, we can try just to have more brute-force approach. This would be to go to character n-grams. This is FastText model proposed by Facebook research, and this is really famous just because it has a good implementation and you can play with it. So, the idea is as follows. Let us represent a word by a set of character n-grams, and also let us put the word itself to this set as well. For example, for character n-grams of size three, we'll have this example in this slide. Usually, we will have several n-values, like n from three to six. And we will have n-grams of different length in the same set, and this set will represent the word. Now, how can we use this? Well, if you remember in skip-gram negative sampling, we had some similarity between two words and we could represent it just by those product of these words. Now, what if the words are represented not by vectors but by set of vectors? Well, we can sum. So, we can say that now we have a sum over all those character n-grams, and every character n-gram is represented by the vector. Awesome. So, I think the idea is great, and it works well for languages with rich morphology. So, FastText model provides a nice way to represent sub-word unions. Now, what if we need to go to another level and to represent sentences? Do you have any ideas how to build sentence embeddings? There are some ideas summarized in this slide. So, the more simple ideas would be, what if we just take the pre-trained vectors, let's say, from Word2Vec model and average them to obtain the embedding of the sentence? Well, you might have also some weighted average, for example with TF-IDF weights. But, you know what? It might be not too nice approach because those pre-trained vectors are trained with some other objectives, and they might not suit well for our task. So, another idea would be somehow to represent the sentences as a sum of sub-sentence units. Let's have a closer look. First, we are going to represent the similarity between word and the sentence, and our training data will be those words that occur in some sentences. So it will be a positive pair, word occurred in a sentence. The negative example will be some word that occurs in some other sentence. So we assume that they are not similar. Now, how do we model this similarity? Again, we have a sum over sub-unions. So, these unions will be word n-grams. And, a bag of word n-grams will represent a sentence. Awesome. So, you see that this model is very similar to FastText model, but instead of having character n-grams to represent words, you have word n-grams to represent sentences. Also, another minor difference is that now you have average not the sum. We have this one divided by the size of the set, but this is not so important. So you see that in different levels of our language we can have some similar ideas. What if we build some general approach for all these levels? An attempt to build this general approach is found in a very recent paper which is called StarSpace. So, the idea here is that we have some entities represented by features. For example, words represented by character n-grams or sentences represented by word n-grams. But you can go further. For example, we can think about recommendation systems. There we have users, and they are represented as bag of items that they like. For example, as bag of movies. So we'll learn how to embed users and movies in the same space. Another example, it would be document classification problem. So there you have documents as a bag of words and you have labels, for example, sentiment labels, and these are rather simple entities that are represented by a singleton feature, the label itself. So, in this application, you will try to learn to produce correct labels for document. So, you'll say that the similarity measure between the documents and the label should be high if this label can be found in the supervised data for this document, and low vice-versa. So you build the model and you get the embeddings for labels, and documents, and words in the same space. Now, you can read about all those applications on the GitHub page. But I want to cover in more details just one application. And this will be, again, sentence embeddings. So let's say, we have some supervised data about similar sentences. So we know that some group of sentences are duplicates and they are similar. Let us put them into one line of the file. Let us have tabs between the sentences and let us have spaces between the words. Now, in this format, we can feed these data to StarSpace and say that we need to train the embeddings for words and sentences. Then, what happens next? Well, the similar sentences are the good source for positive examples. And we will just take two sentences from the line and use them as a positive example in our model. Now these similar sentences can be just sampled at random, for example we take a sentence from one line and just a random sentence from another line and say that they are a negative example. Then we train those kind of word2vec models in some sense, and obtain the embeddings for all our entities. Awesome. So, the last thing that I want to cover is Deep learning approaches to build sentence representations. So, actually, everything up to this point, we are rather shallow networks. So if we speak about deep learning, we could have three main trends here. One trend would be, obviously, recurrent neural networks that are popular in NLP. Another would be convolutional neural networks that are actually much faster than recurrent neural networks so it seems like it is a super promising approach. And the third one would be recursive neural networks or so-called Tree-LSTMs or Dynamic Acyclic Graph, DAG-LSTM. So, these kind of models use the syntax of the language to build some hierarchical representations. These are rather awesome approaches. We will not have too much time to cover them, but you just need to know that syntax can help us to build the representation of the sentence. Now, the take-away of this slide would be that linguistics can help us, for example as morphology syntax in many many tasks. The last architecture that I want to cover in this video is called skip-thought vectors. And it is based on recurrent neural networks. So the idea is as follows. You have some sentence, and you want to predict the next sentence. You encode your sentence with a recurrent neural network and get some hidden representation. It is called thought vector. Now, once you have these, you try to generate the next sentence with the language model. So you already know that there are neural language models. Now it is a conditional neural language model conditioned on this thought vector. And the great thing is that this thought vector is going to represent the meaning of the sentence, and it can be used as the embedding. Actually, this architecture is called encoder-decoder architecture. And we will have many many details about it in the next week. So, if you haven't realized all the details just from one slide, don't worry, we will cover them in many many details.